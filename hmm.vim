隐马尔可夫模型简介
一、引入
1. 状态机
状态机举例：计数器

要素：
1.状态及其个数
2.状态转移的规律
3.初始状态


2.马尔可夫模型

马尔可夫模型举例：天气转换

马尔可夫模型要素
1.状态及其个数
2.状态转移的规律
展开：状态转移概率与上面的状态机相比有所不同，上面的转换关系从某种程度上来说是确定，马尔可夫模型的转换关系是随机的 只能通过概率的形式来描述这种状态之间的联系
3.初始状态
 

马尔可夫模型的含义
马尔可夫其实是对现实按时间顺序发生的并且相互关联的一类事件的抽象化和简单化，这可以体现在马尔可夫的两个假设上：
1.当前时刻发生事件的结果只受前面有限个时刻的事件结果的影响（关于这个假设值得一提 有限（蝴蝶效应+模型阶数的含义） 之前（也就是说过去和未来是无关的 非因果系统））
2.状态之间的这种关联不随时间的改变而改变（也即平稳性）

这些假设一方面简化了对一些事件的研究难度，当然另一方面也难免丢失了一部分信息。


3.隐马尔可夫模型：


马尔可夫模型还有可以改进的地方

隐马尔可夫模型图
改变之处在于状态转换关系往下一移了一层 变成隐状态（顾名思义，隐状态就是我们无法直接观察到的隐藏状态）然后每个隐状态都根据一定的概率发出一些显性符号 而这些符号才是我们能够观察到的状态

为什么要进行这样的改进
一方面有时候我们并不满足分析事物表面层面上的关系想要进一步了解隐藏在更深层次的规律（主观因素层面）
另一方面很多情况下往往无法从表象上观察到规律，联系往往隐藏于表象之下（客观制约）

隐马尔可夫模型要素
前面的要素
增加了一个混淆矩阵 其实就是给定一个隐状态后得到显状态的概率矩阵 同样的也有类似的平稳性假设 即这个概率矩阵也是不随时间改变的


隐马尔可夫的应用



到这里可能很多同学还是对隐马尔可夫的隐字没什么概念
我们在这里就举一个语音识别的例子来进一步了解
我们说出的话对于计算机来说就是一段语音数据 其实真实的含义计算机是无法直接获得的 而只能猜测
在这个例子中语音数据就是显状态 实际的含义就是隐藏在语音的文字

实际中 隐马尔可夫模型在语音识别 词性标注等方面扮演着极其重要的角色




4.小结 
那么到这里我们可以做一个小结
马尔可夫模型和前面我们学到的决策情形有所不同 它更注重于对一些根据时间顺序依次发生的事件的分析 分析的角度主要是根据前后事件的相互联系 而前面学到的比如说鲑鱼和鲈鱼的分类例子，事件虽然说好像也是依序发生的，但是前后事件之间是服从一定概率而相互独立发生的（也就是说事件之间没有什么联系） 


二、三个基本问题
（接下来进入本节的重点，我们学习一下隐马尔可夫模型的三个基本问题以及在解决这三个问题过程中用到的一些算法）
1.评估问题
这个问题的主要工作是在某个已知的马尔可夫模型下推测一组特定的显状态序列的出现概率
（1）题目举例
看到这个问题 我们可能想到的第一个是穷举法，就是将所有可能的隐序列及序列产生的概率列出来 再将每一个隐状态序列生成对应的（用例子说明）
前向算法 优点：采用了递归的思想 大大降低了运算复杂度
（2）问题意义
如果我们现在需要通过语音识别几个英文单词 每个英文单词包含数个音节（显状态），而且我们已经构建好了这几个单词的隐马尔可夫模型，现在有电脑通过语音识别出来了一段音节序列，如果我们比较这几个马尔可夫模型产生这个音节序列的概率并选取一个最大的，就可以推断出最可能的单词，这就是语音识别的基本原理。
2.解码问题 
如果现在有一个显状态序列，我们希望推断出其最可能对应的隐状态序列，这就是解码问题
（1）题目举例
vertibi算法




（2）注意事项 
其实问题到这里就已经解决了但是我们看一下选出来的最佳路径就会发现这条路径其实是不可用的。。。。

那怎么办呢 我们只能另辟蹊径了

由此我们可以总结出这么一点：
根据维特比算法算出来的最佳路径并不一定是可行的 因此我们在算完之后一定要进行检查


3.学习问题
前面我们提到的两个问题都需要一个前提 那就是马尔可夫模型及其参数是要确定的 已知的 
但是在实际中我们更常遇到的一种情况是只有一些训练样本，而马尔可夫模型是未知的 确定最符合最符合已有训练样本的马尔可夫模型参数（\pi A B）就是学习问题

学习问题的算法通常无法直接求得最优解 而需要根据前向后向算法（forward backward algorithm） 通过迭代的方式一步步地毕竟最佳值

前向算法前面已经提过了 很简单 后向算法跟前向算法的基本原理是一样的 就是将计算的顺序颠倒一下 改为从序列的后端开始一步步往前算
而前向后向算法结合了前向和后向算法 

因为这个算法比较公式比较多 这里就只讲一下基本过程
定义变量
α（t，wi）表示使系统在t时刻隐状态处于wi 并产生了到t时刻为止（包括t时刻）的显序列的概率
跟前向算法是相关的，
β（t,wi）表示系统在t时刻隐状态处于wi状态 并且已经产生了t时刻之后（不包括t时刻）的显序列的概率 跟后向算法相关
γ（t-1,wi,t,wj）表示系统在由t-1时刻的wi隐状态转移到t时刻的wj隐状态的概率 它起到了连接了前后向算法
然后就有这样一个公式 
γ（t-1,wi,t,wj）= ....
然后又有一个公式

这个公式可以对α和β进行改善 生成更佳的估计

这两个公式看起来有点吓人 但是用到的还是比较简单的概率原理，大家有兴趣课后可以推一下

根据这两个公式我们如果设置一个初始值  就可以根据训练样本来一步步地优化 当这个值与最佳值得差别足够小的时候 停止迭代 就可以得到 足够逼近最佳的α和β

根据α和β我们又能得到这个马尔可夫模型的参数，问题就解决了

三、总结
今天我们主要了解了隐马尔可夫模型的一些基本概念以及由其引申出来的3个基本问题 我们发现 其实它的原理还是比较简单 比较容易理解的
隐马尔可夫模型更适合对一些根据时间顺序依次发生的事件的分析 这个特点使它在语音识别 词性标注等方面占有重要地位 但是究其本质 它依然是按照概率的最大化的原则来进行决策分析的，所以它依旧是属于统计模式识别的一部分。




